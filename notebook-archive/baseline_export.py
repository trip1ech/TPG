#!/usr/bin/env python
# coding: utf-8

# In[1]:


# A better way to read the same file, handling BOM and end-of-line truncation

import re
import pandas as pd
from pathlib import Path

path = "arrets_ben.csv"
out_parquet = "arrets_ben.parquet"

# --- 1) Read header + dash line, remove BOM automatically ---
with open(path, encoding="utf-8-sig") as f:
    header_line = f.readline().rstrip("\n")
    dash_line   = f.readline().rstrip("\n")

# --- 2) Infer column spans from runs of dashes ---
colspecs = [(m.start(), m.end()) for m in re.finditer(r"-+", dash_line)]

# Make the last column go to end-of-line to avoid truncation
colspecs[-1] = (colspecs[-1][0], None)

# --- 3) Slice column names from the (BOM-stripped) header ---
raw_names = [header_line[s:] if e is None else header_line[s:e] for s, e in colspecs]
names = []
seen = {}
for nm in map(str.strip, raw_names):
    seen[nm] = seen.get(nm, -1) + 1
    names.append(nm if seen[nm] == 0 else f"{nm}_{seen[nm]}")

print("Detected columns:", len(names))
print(names[:10], "...")

# --- 4) Read the data as fixed-width (skip header + dashes) ---
df = pd.read_fwf(
    path,
    colspecs=colspecs,
    names=names,
    skiprows=2,
    na_values=["NULL"],
    encoding="utf-8-sig",
)
print(df.shape)
print(df.head(3))


# ### Build Segment

# In[2]:


# =========================
# Build `seg` (prev stop -> current stop) from df
# - single-source targets:
#     link_s  := TempsInterArretRealise               (actual link runtime)
#     dwell_s := DTSortieFenetreArretReal - DTEntreeFenetreArretReal (window-based dwell)
# - punctuality signals (if present):
#     E_i, E_prev, dE := EcartDepart and its lag
# - time anchor for features:
#     link_start_time := previous stop's window-exit (fallback to ATP enter, then schedule depart)
# =========================
import numpy as np
import pandas as pd

seg_src = df.copy()

# --- 0) Parse datetimes (only if columns exist) ---
time_cols = [
    "DTDepartTheo","DTArriveeTheo",
    "DTEntreeFenetreArretReal","DTSortieFenetreArretReal",
    "DTEntreeArretAtp","DTSortieArretAtp",
    "DTMarquageArretTheo","DTMarquageArretReal",
    "HOuverturePortesReal","HFermetureportesReal"
]
for c in time_cols:
    if c in seg_src.columns:
        seg_src[c] = pd.to_datetime(seg_src[c], errors="coerce")

# --- 1) Basic casting / trimming ---
# IDs & ordering
if "RangArretAsc" in seg_src.columns:
    seg_src["RangArretAsc"] = pd.to_numeric(seg_src["RangArretAsc"], errors="coerce")
if "C_Ligne" in seg_src.columns:
    seg_src["C_Ligne"] = pd.to_numeric(seg_src["C_Ligne"], errors="coerce")

# stop codes / direction as clean strings
seg_src["CodeLong"] = seg_src.get("CodeLong", "").astype(str).str.strip()
if "C_SensAppl" in seg_src.columns:
    seg_src["C_SensAppl"] = seg_src["C_SensAppl"].astype(str).str.strip()

# distances & counts (if exist)
for c in ["DistanceInterArret","TempsInterArretRealise","EcartDepart","NbMontees","NbDescentes"]:
    if c in seg_src.columns:
        seg_src[c] = pd.to_numeric(seg_src[c], errors="coerce")

# --- 2) Sort and build previous-stop columns (within a trip/course) ---
by = ["IdCourse","RangArretAsc"] if "IdCourse" in seg_src.columns else ["C_Ligne","DateCourse","RangArretAsc"]
seg_src = seg_src.sort_values(by, kind="mergesort")

grp = seg_src.groupby("IdCourse") if "IdCourse" in seg_src.columns else seg_src.groupby(["C_Ligne","DateCourse"])

seg_src["prev_CodeLong"] = grp["CodeLong"].shift(1)

# previous-window exit (our preferred "actual depart" at upstream stop)
if {"DTEntreeFenetreArretReal","DTSortieFenetreArretReal"}.issubset(seg_src.columns):
    seg_src["prev_DT_win_out"] = grp["DTSortieFenetreArretReal"].shift(1)
    seg_src["prev_DT_win_in"]  = grp["DTEntreeFenetreArretReal"].shift(1)
else:
    seg_src["prev_DT_win_out"] = np.nan
    seg_src["prev_DT_win_in"]  = np.nan

# fallbacks if window missing: ATP enter, then scheduled depart
seg_src["prev_DT_atp_in"]   = grp["DTEntreeArretAtp"].shift(1) if "DTEntreeArretAtp" in seg_src.columns else np.nan
seg_src["prev_DT_sched_dep"]= grp["DTDepartTheo"].shift(1)     if "DTDepartTheo" in seg_src.columns else np.nan

# upstream demand / punctuality
for c in ["NbMontees","NbDescentes","EcartDepart"]:
    if c in seg_src.columns:
        seg_src[f"prev_{c}"] = grp[c].shift(1)

# --- 3) Keep only rows that have a previous stop (i.e., valid link) ---
seg = seg_src.dropna(subset=["prev_CodeLong"]).copy()
seg["from_stop"] = seg["prev_CodeLong"].astype(str).str.strip()
seg["to_stop"]   = seg["CodeLong"].astype(str).str.strip()
seg["SegmentKey"] = seg["from_stop"] + "â†’" + seg["to_stop"]

# --- 4) Targets from single sources ---
# 4a) Actual link runtime: Realise (single source; no cross-source mixing)
seg["link_s"] = seg.get("TempsInterArretRealise")
seg.loc[(seg["link_s"]<=0) | (seg["link_s"]>1800), "link_s"] = np.nan  # basic QC (0/neg, >30min)

# 4b) Dwell time from window timestamps (single source)
if {"DTEntreeFenetreArretReal","DTSortieFenetreArretReal"}.issubset(seg.columns):
    seg["dwell_s"] = (seg["DTSortieFenetreArretReal"] - seg["DTEntreeFenetreArretReal"]).dt.total_seconds()
    seg.loc[(seg["dwell_s"]<0) | (seg["dwell_s"]>900), "dwell_s"] = np.nan  # QC: <0 or >15min
else:
    seg["dwell_s"] = np.nan

# 4c) Punctuality signals (if present)
seg["E_i"]    = pd.to_numeric(seg.get("EcartDepart"), errors="coerce")
seg["E_prev"] = pd.to_numeric(seg.get("prev_EcartDepart"), errors="coerce")
seg["dE"]     = seg["E_i"] - seg["E_prev"]

# --- 5) Upstream dwell used as a feature (window-based) ---
seg["dwell_prev_s"] = (seg["prev_DT_win_out"] - seg["prev_DT_win_in"]).dt.total_seconds()
seg.loc[(seg["dwell_prev_s"]<0) | (seg["dwell_prev_s"]>900), "dwell_prev_s"] = np.nan

# --- 6) Distance feature (static meta; keep raw to avoid mixing assumptions) ---
seg["distance_m"] = pd.to_numeric(seg.get("DistanceInterArret"), errors="coerce")
seg.loc[seg["distance_m"]<0, "distance_m"] = np.nan

# --- 7) Time anchor for features: previous stop's window-exit (fallbacks) ---
seg["link_start_time"] = seg["prev_DT_win_out"]
seg.loc[seg["link_start_time"].isna(), "link_start_time"] = seg["prev_DT_atp_in"]
seg.loc[seg["link_start_time"].isna(), "link_start_time"] = seg["prev_DT_sched_dep"]

# Drop rows without any anchor time
seg = seg[pd.notna(seg["link_start_time"])].copy()

# Calendar & cyclical features
seg["hour"] = seg["link_start_time"].dt.hour
seg["dow"]  = seg["link_start_time"].dt.dayofweek   # 0=Mon .. 6=Sun
seg["is_weekend"] = seg["dow"].isin([5,6]).astype(int)
seg["hour_sin"] = np.sin(2*np.pi*seg["hour"]/24.0)
seg["hour_cos"] = np.cos(2*np.pi*seg["hour"]/24.0)
seg["period168"] = seg["dow"]*24 + seg["hour"]      # 0..167

# Optional coarse label (handy for plots)
def assign_period(dt):
    h, d = dt.hour, dt.dayofweek
    if d==5: return "Sat"
    if d==6: return "Sun"
    if 7<=h<9:   return "AM"
    if 9<=h<16:  return "Day"
    if 16<=h<19: return "PM"
    if 19<=h<23: return "Eve"
    return "Other"
seg["period"] = seg["link_start_time"].map(assign_period)

# --- 8) IDs for grouping/filters ---
seg["line"] = seg.get("C_Ligne").astype("Int64").astype(str) if "C_Ligne" in seg.columns else "NA"
seg["dir"]  = seg.get("C_SensAppl", "NA").astype(str)

# demand features from upstream stop (if present)
seg["board_prev"]  = pd.to_numeric(seg.get("prev_NbMontees"),   errors="coerce")
seg["alight_prev"] = pd.to_numeric(seg.get("prev_NbDescentes"), errors="coerce")

# --- 9) Feature list for GBM (E/Î”E) ---
feat_cols = [
    "distance_m","dwell_prev_s","E_prev",
    "board_prev","alight_prev",
    "hour_sin","hour_cos","dow","is_weekend",
    "from_stop","to_stop","line","dir"
]

print(f"[seg] rows: {len(seg):,}")
print("Targets available:",
      f"link_s={seg['link_s'].notna().mean():.2%}",
      f"dwell_s={seg['dwell_s'].notna().mean():.2%}",
      f"E_i={seg['E_i'].notna().mean():.2%}",
      f"E_prev={seg['E_prev'].notna().mean():.2%}",
      f"dE={seg['dE'].notna().mean():.2%}")
print("Feature columns:", feat_cols)


# In[3]:


# Assumes you already have `seg` with:
# - line, from_stop, to_stop
# - link_start_time (datetime)
# - TempsInterArretRealise (actual link runtime)
# - DTEntreeFenetreArretReal, DTSortieFenetreArretReal (window timestamps)

# 1) segment key (include line to avoid cross-line collisions)
seg = seg.copy()
seg["seg3"] = seg["line"].astype(str) + "|" + seg["from_stop"] + "â†’" + seg["to_stop"]

# 2) actual link runtime (single source: Realise)
seg["link_s"] = pd.to_numeric(seg.get("TempsInterArretRealise"), errors="coerce")

# 3) dwell (single source: window)
if {"DTEntreeFenetreArretReal","DTSortieFenetreArretReal"}.issubset(seg.columns):
    seg["dwell_s"] = (
        seg["DTSortieFenetreArretReal"] - seg["DTEntreeFenetreArretReal"]
    ).dt.total_seconds()
else:
    seg["dwell_s"] = np.nan

# 4) basic QC (leave NaN if missing; remove impossible/outlier tails conservatively)
seg.loc[seg["link_s"]<=0, "link_s"] = np.nan
seg.loc[seg["link_s"]>1800, "link_s"] = np.nan  # 30 min hard cap (tune)
seg.loc[seg["dwell_s"]<0, "dwell_s"] = np.nan
seg.loc[seg["dwell_s"]>900, "dwell_s"] = np.nan  # 15 min hard cap (tune)

# 5) time bins: hour-of-day Ã— day-of-week -> 168 buckets
ts = seg["link_start_time"]
seg["dow"]   = ts.dt.dayofweek      # 0=Mon..6=Sun
seg["hour"]  = ts.dt.hour           # 0..23
seg["period168"] = seg["dow"]*24 + seg["hour"]  # 0..167


# In[4]:


# =========================
# Empirical baselines for link_s / dwell_s
# - train window â†’ fit quantile maps (by levels)
# - test window  â†’ apply with hierarchical fallback
# =========================
import numpy as np
import pandas as pd

# 1) æ™‚é–“åˆ‡åˆ†ï¼ˆæ²¿ç”¨ä½ çš„ splitï¼‰
TRAIN_FROM = "2024-10-01"
TRAIN_TO   = "2024-10-31"
TEST_FROM  = "2024-11-01"
TEST_TO    = "2024-12-31"

mask_tr = (seg["link_start_time"]>=pd.to_datetime(TRAIN_FROM)) & (seg["link_start_time"]<=pd.to_datetime(TRAIN_TO))
mask_te = (seg["link_start_time"]>=pd.to_datetime(TEST_FROM))  & (seg["link_start_time"]<=pd.to_datetime(TEST_TO))
train = seg[mask_tr].copy()
test  = seg[mask_te].copy()

# 2) å®šç¾©åˆ†å±¤å±¤ç´šï¼ˆç”±ç´°åˆ°ç²—ï¼‰
#    ä½ å¯ä»¥ä¾è³‡æ–™é‡èª¿æ•´ï¼›æ¨£æœ¬ä¸è¶³æ™‚æœƒå¾€ä¸‹ä¸€å±¤å›é€€
LEVELS = [
    ["SegmentKey","period168"],           # æ¯æ®µÃ—æ¯é€±168å°æ™‚
#    ["SegmentKey","is_weekend","hour"],   # æ¯æ®µÃ—æ˜¯å¦é€±æœ«Ã—å°æ™‚
#    ["SegmentKey"],                       # åªçœ‹æ®µ
]
MIN_N = 10  # æ¯ç¾¤è‡³å°‘æ¨£æœ¬æ•¸ï¼›ä¸å¤ å°±å›é€€

def _fit_level_quantile(train_df, target, by, q):
    g = (train_df
         .dropna(subset=[target])
         .groupby(by)[target]
         .agg(n="size", q=lambda s: float(s.quantile(q))))
    # åªç•™ n >= MIN_N çš„ç¾¤
    g = g[g["n"] >= MIN_N].reset_index()
    return g  # columns: by... + ["n","q"]

def fit_maps(train_df, target, levels, q):
    """å›å‚³æ¯ä¸€å±¤çš„ quantile å°ç…§è¡¨ï¼ˆå« nï¼‰èˆ‡å…¨åŸŸä¿åº• quantileã€‚"""
    maps = [ _fit_level_quantile(train_df, target, by, q) for by in levels ]
    global_q = float(train_df[target].dropna().quantile(q)) if train_df[target].notna().any() else 0.0
    return maps, global_q

def apply_maps(apply_df, levels, maps, global_q):
    """ä¾åºå›å¡«ï¼šç¬¬ä¸€å±¤æ²’å‘½ä¸­ â†’ ç¬¬äºŒå±¤ â†’ ... â†’ å…¨åŸŸã€‚ä¹Ÿå›å‚³ç”¨åˆ°å“ªä¸€å±¤ã€‚"""
    pred  = pd.Series(np.nan, index=apply_df.index, dtype=float)
    used  = pd.Series(np.nan, index=apply_df.index)  # ç´€éŒ„å‘½ä¸­å±¤ç´šç´¢å¼•ï¼ˆ0=æœ€ç´°ï¼‰
    remain = pred.isna()

    for i, by in enumerate(levels):
        if len(maps[i]) == 0 or not remain.any():
            continue
        tmp = (apply_df.loc[remain, by]
               .merge(maps[i], on=by, how="left")["q"])
        hit = tmp.notna().values
        pred.loc[remain[remain].index[hit]] = tmp[hit].values
        used.loc[remain[remain].index[hit]] = i
        remain = pred.isna()

    # å…¨åŸŸä¿åº•
    pred.loc[remain] = global_q
    used.loc[remain] = len(levels)  # å…¨åŸŸ
    return pred.values, used.values

def make_empirical_baseline(train_df, test_df, target, q, levels=LEVELS):
    maps, gq = fit_maps(train_df, target, levels, q)
    pred, used = apply_maps(test_df, levels, maps, gq)
    return pred, used, maps, gq

# 3) å° link_s / dwell_s åš p50 èˆ‡ p85 baseline
# Link (æ®µé–“å¯¦æ¸¬è¡Œè»Šæ™‚é–“)
b_link50, usedL50, mapsL50, gL50 = make_empirical_baseline(train, test, target="link_s", q=0.50, levels=LEVELS)
b_link85, usedL85, mapsL85, gL85 = make_empirical_baseline(train, test, target="link_s", q=0.85, levels=LEVELS)

# Dwellï¼ˆç«™å…§åœç•™ï¼›è‹¥ window ä¸å®Œæ•´å¯èƒ½è¼ƒç¨€ç–ï¼‰
b_dwell50, usedD50, mapsD50, gD50 = make_empirical_baseline(train, test, target="dwell_s", q=0.50, levels=LEVELS)
b_dwell85, usedD85, mapsD85, gD85 = make_empirical_baseline(train, test, target="dwell_s", q=0.85, levels=LEVELS)

# 4) ç°¡å–®è©•ä¼°ï¼ˆp50 ç•¶é»ä¼°ã€p85 ç•¶è¦†è“‹ï¼‰
def mae(a, b):
    m = np.isfinite(a) & np.isfinite(b)
    return float(np.mean(np.abs(a[m] - b[m]))) if m.any() else np.nan

def coverage(y, qhat):
    m = np.isfinite(y) & np.isfinite(qhat)
    return float(np.mean(y[m] <= qhat[m])) if m.any() else np.nan

print("=== Link_s baselines ===")
print("MAE vs p50:", mae(test["link_s"].to_numpy(float), b_link50))
print("Cov@p85  :", coverage(test["link_s"].to_numpy(float), b_link85))
print("Hit-rate by level (p50):", pd.Series(usedL50).value_counts(normalize=True).sort_index().to_dict())

print("\n=== Dwell_s baselines ===")
print("MAE vs p50:", mae(test["dwell_s"].to_numpy(float), b_dwell50))
print("Cov@p85  :", coverage(test["dwell_s"].to_numpy(float), b_dwell85))
print("Hit-rate by level (p50):", pd.Series(usedD50).value_counts(normalize=True).sort_index().to_dict())

# 5) ç”¢ç”Ÿã€Œempirical deviationã€å¯ä¾›å¾ŒçºŒåˆ†æ/é æ¸¬ï¼ˆå®Œå…¨ä¸ç¢°ç†è«–ï¼‰
test = test.copy()
test["link_dev_emp_p50"]  = test["link_s"]  - b_link50   # æ­£å€¼ = æ¯”å¸¸æ…‹æ…¢ï¼ˆå»¶ï¼‰
test["dwell_dev_emp_p50"] = test["dwell_s"] - b_dwell50

# ï¼ˆå¯é¸ï¼‰æŠŠ baseline é™„å› testï¼Œä¹‹å¾Œåšåœ–æˆ–ç•¶ ML feature çš†å¯
test["base_link_p50"]  = b_link50
test["base_link_p85"]  = b_link85
test["base_dwell_p50"] = b_dwell50
test["base_dwell_p85"] = b_dwell85


# In[5]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter

# --- helpers ---
def _ensure_link_s(seg):
    if "link_s" not in seg.columns:
        cand = None
        for c in ["TempsInterArretRealise","act_link_s"]:
            if c in seg.columns: cand = c; break
        if cand is None:
            raise ValueError("need link_s / TempsInterArretRealise / act_link_s")
        seg = seg.copy()
        seg["link_s"] = pd.to_numeric(seg[cand], errors="coerce")
    return seg

def _segment_spread_table(s):
    g = s.groupby("SegmentKey")["link_s"]
    tbl = pd.DataFrame({
        "n":      g.size(),
        "median": g.median(),
        "p10":    g.quantile(0.10),
        "p25":    g.quantile(0.25),
        "p75":    g.quantile(0.75),
        "p90":    g.quantile(0.90),
    })
    tbl["IQR"] = tbl["p75"] - tbl["p25"]
    tbl["p90-p10"] = tbl["p90"] - tbl["p10"]
    return tbl

def _count_breaks(ordered_keys):
    """é€£çºŒæ€§æª¢æŸ¥ï¼šç›¸é„°æ®µæ˜¯å¦ to==next.from"""
    def ends(k): 
        a,b = k.split("â†’"); return a,b
    br = 0
    for k1, k2 in zip(ordered_keys, ordered_keys[1:]):
        if ends(k1)[1] != ends(k2)[0]:
            br += 1
    return br

def _top_patterns(seg_line_dir, top_k=2, min_share=0.10):
    """
    ä»¥æ¯å€‹ç­æ¬¡çš„ã€Œå®Œæ•´åœé åºåˆ—ã€ç•¶ä½œ patternï¼š
    path = [è©²ç­æ¬¡ç¬¬ä¸€ç­† seg çš„ from_stop] + list(to_stop)
    å›å‚³ [(pattern_tuple, count, share), ...] (æœ€å¤š top_kï¼Œä¸” share >= min_share)
    """
    s = seg_line_dir.copy()

    # ç”¨ç­æ¬¡å…§çš„åœé é †åºæ’åºï¼šå„ªå…ˆç”¨ RangArretAscï¼›æ²’æœ‰å°±é€€å› link_start_time
    if "RangArretAsc" in s.columns:
        s = s.sort_values(["IdCourse", "RangArretAsc"], kind="mergesort")
    else:
        s = s.sort_values(["IdCourse", "link_start_time"], kind="mergesort")

    # ä¹¾æ·¨çš„ stop id
    if "from_stop" not in s.columns or "to_stop" not in s.columns:
        # è¬ä¸€æ²’æœ‰ from/to_stopï¼Œå°±å¾ SegmentKey é‚„åŸ
        a = s["SegmentKey"].str.split("â†’", expand=True)
        s["from_stop"] = a[0].astype(str).str.strip()
        s["to_stop"]   = a[1].astype(str).str.strip()
    else:
        s["from_stop"] = s["from_stop"].astype(str).str.strip()
        s["to_stop"]   = s["to_stop"].astype(str).str.strip()

    # çµ„å‡ºæ¯å€‹ç­æ¬¡çš„å®Œæ•´è·¯å¾‘ï¼ˆå«èµ·é»ï¼‰
    def _trip_full_path(g):
        first_from = g["from_stop"].iloc[0]
        to_seq     = g["to_stop"].tolist()
        return tuple([first_from] + to_seq)

    paths = s.groupby("IdCourse", sort=False).apply(_trip_full_path)

    cnt = Counter(paths)
    total = sum(cnt.values()) if cnt else 0
    ranked = [(p, n, n/total) for p, n in cnt.most_common() if total > 0 and (n/total) >= min_share]
    return ranked[:top_k]

def plot_line_by_patterns(seg, seg_pat, line, dir_=None, top_k=2, min_share=0.10,
                          min_n_per_seg=20, per_fig_max_segments=24, figsize=(16,6)):
    # seg ç”¨æ–¼ç•«å€¼ï¼ˆåŒ…å« link_s æ­£å€¼èˆ‡ QCï¼‰
    s_pos = _ensure_link_s(seg)
    s_pos = s_pos[s_pos["line"].astype(str) == str(line)].copy()
    if dir_ is not None:
        s_pos = s_pos[s_pos["dir"].astype(str) == str(dir_)].copy()
    s_pos = s_pos[pd.to_numeric(s_pos["link_s"], errors="coerce") > 0].copy()
    if s_pos.empty:
        print("no data with positive link_s"); return

    # y è»¸ç¯„åœç”¨ s_pos
    y_lo = float(np.nanpercentile(s_pos["link_s"], 1))
    y_hi = float(np.nanpercentile(s_pos["link_s"], 99))

    # ğŸ”‘ pattern ç”¨ seg_patï¼ˆä¸æœƒæ¼æ‰èµ·ç«™çš„ç¬¬ä¸€æ®µï¼‰
    s_all = seg_pat[(seg_pat["line"].astype(str)==str(line)) & (seg_pat["from_stop"].notna())].copy()
    if dir_ is not None:
        s_all = s_all[s_all["dir"].astype(str) == str(dir_)].copy()

    pats = _top_patterns(s_all, top_k=top_k, min_share=min_share)
    if not pats:
        print("no dominant patterns; try lowering min_share"); return

    results = {}
    for pi,(pat, n_pat, share) in enumerate(pats, start=1):
        ordered_keys = [f"{a}â†’{b}" for a,b in zip(pat[:-1], pat[1:])]

        # åªæ‹¿ä¾†ç•«å€¼çš„è³‡æ–™ï¼ˆç”¨æœ‰æ­£å€¼ link_s çš„ segï¼‰
        sp = s_pos[s_pos["SegmentKey"].isin(set(ordered_keys))].copy()

        # å…è¨±å‰ 1â€“2 æ®µæ¨£æœ¬å°‘ä¹Ÿä¿ç•™ï¼ˆé¿å…èµ·ç«™è¢« min_n åƒæ‰ï¼‰
        vc = sp["SegmentKey"].value_counts()
        head_keep = set(ordered_keys[:2])
        ok = set(vc[vc >= min_n_per_seg].index) | (head_keep & set(vc.index))
        ordered_keys = [k for k in ordered_keys if k in ok]
        sp = sp[sp["SegmentKey"].isin(ordered_keys)]

        if not ordered_keys:
            print(f"pattern#{pi}: all segments < min_n; skip"); continue

        # æ–·é»çµ±è¨ˆ
        breaks = _count_breaks(ordered_keys)

        # ç•«åœ–ï¼ˆåŒä½ åŸæœ¬ï¼‰
        def _chunk(lst, n):
            for i in range(0, len(lst), n):
                yield lst[i:i+n]

        print(f"\nPattern #{pi}: share={share:.1%}, trips={n_pat}, segments_kept={len(ordered_keys)}, breaks={breaks}")

        spread = _segment_spread_table(sp)
        page = 1
        for keys in _chunk(ordered_keys, per_fig_max_segments):
            data = [sp.loc[sp["SegmentKey"]==k, "link_s"].values for k in keys]
            fig, ax = plt.subplots(figsize=figsize)
            ax.boxplot(data, vert=True, showfliers=False, widths=0.6, labels=keys)
            ax.set_ylim(y_lo, y_hi)
            ttl = f"Line {line}"
            if dir_ is not None: ttl += f" | dir={dir_}"
            ttl += f" â€” pattern #{pi} (page {page})"
            ax.set_title(ttl)
            ax.set_ylabel("link_s (s)")
            ax.grid(True, linestyle="--", alpha=0.3, axis="y")
            ax.tick_params(axis='x', rotation=75)
            plt.tight_layout(); plt.show()
            page += 1

        results[f"pattern_{pi}"] = {
            "share": share, "trips": n_pat, "ordered_keys": ordered_keys,
            "breaks": breaks, "spread": spread.loc[ordered_keys]
        }
    return results



# ---- ä½¿ç”¨ä¾‹ ----
# åªç•« 18 è™Ÿç·š A å‘ï¼Œå‰ 2 å€‹ä¸»æµ patternï¼Œæ¯å€‹ pattern åªä¿ç•™æ¨£æœ¬æ•¸ >= 20 çš„æ®µ
#res18A = plot_line_by_patterns(seg=seg, seg_pat=seg_pat,
#                               line=18, dir_="A",
#                               top_k=2, min_share=0.15,
#                               min_n_per_seg=20, per_fig_max_segments=20, figsize=(17,6))
# çœ‹ pattern #1 çš„é›†ä¸­/åˆ†æ•£è¡¨
# res18A["pattern_1"]["spread"]

def extract_patterns(seg_pat, line, dir_=None, min_share=0.10):
    """
    Return a table of dominant stop-order patterns (>= min_share) for a given line[/dir].
    seg_pat must contain: IdCourse, line, dir, from_stop, to_stop (as in your code).
    """
    s = seg_pat.copy()
    s = s[s["line"].astype(str) == str(line)]
    if dir_ is not None:
        s = s[s["dir"].astype(str) == str(dir_)]
    if s.empty:
        return pd.DataFrame(columns=["pattern_id","trips","share","n_segments","start","end","pattern_str","preview"])

    # Order rows within trip: prefer RangArretAsc, fallback to link_start_time
    if "RangArretAsc" in s.columns:
        s = s.sort_values(["IdCourse", "RangArretAsc"], kind="mergesort")
    else:
        s = s.sort_values(["IdCourse", "link_start_time"], kind="mergesort")

    # Ensure from/to exist
    if "from_stop" not in s.columns or "to_stop" not in s.columns:
        a = s["SegmentKey"].str.split("â†’", expand=True)
        s["from_stop"] = a[0].astype(str).str.strip()
        s["to_stop"]   = a[1].astype(str).str.strip()
    else:
        s["from_stop"] = s["from_stop"].astype(str).str.strip()
        s["to_stop"]   = s["to_stop"].astype(str).str.strip()

    # Build full path per trip: [first_from] + list(to_stop)
    def _trip_full_path(g):
        first_from = g["from_stop"].iloc[0]
        to_seq = g["to_stop"].tolist()
        return tuple([first_from] + to_seq)

    paths = s.groupby("IdCourse", sort=False).apply(_trip_full_path)

    # Count and compute share
    cnt = Counter(paths)
    total = sum(cnt.values())
    if total == 0:
        return pd.DataFrame(columns=["pattern_id","trips","share","n_segments","start","end","pattern_str","preview"])

    rows = []
    # Keep only patterns >= min_share
    ranked = [(p, n, n/total) for p, n in cnt.most_common() if (n/total) >= min_share]
    for i, (pat, n, share) in enumerate(ranked, start=1):
        pat_list = list(pat)
        n_segments = max(len(pat_list)-1, 0)
        start = pat_list[0] if pat_list else ""
        end   = pat_list[-1] if pat_list else ""
        # full string version
        pattern_str = " â†’ ".join(pat_list)
        # short preview (first 3 â€¦ last 3, adaptable)
        if len(pat_list) <= 8:
            preview = pattern_str
        else:
            preview = " â†’ ".join(pat_list[:3]) + " â†’ â€¦ â†’ " + " â†’ ".join(pat_list[-3:])
        rows.append({
            "pattern_id": i,
            "trips": n,
            "share": round(share, 4),
            "n_segments": n_segments,
            "start": start,
            "end": end,
            "pattern_str": pattern_str,
            "preview": preview
        })

    df = pd.DataFrame(rows).sort_values(["share","trips"], ascending=False, kind="mergesort").reset_index(drop=True)
    return df

# å…ˆåˆ°ã€ŒDrop rows without any anchor timeã€ä¹‹å‰ç‚ºæ­¢ï¼Œè¤‡è£½ä¸€ä»½ï¼š
seg_pat = seg_src.dropna(subset=["prev_CodeLong"]).copy()
seg_pat["from_stop"] = seg_pat["prev_CodeLong"].astype(str).str.strip()
seg_pat["to_stop"]   = seg_pat["CodeLong"].astype(str).str.strip()
seg_pat["SegmentKey"] = seg_pat["from_stop"] + "â†’" + seg_pat["to_stop"]
seg_pat["line"] = seg_src.get("C_Ligne").astype("Int64").astype(str)
seg_pat["dir"]  = seg_src.get("C_SensAppl", "NA").astype(str)


# In[6]:


res18A = plot_line_by_patterns(seg=seg, seg_pat=seg_pat,
                              line=18, dir_="A",
                              top_k=2, min_share=0.15,
                              min_n_per_seg=20, per_fig_max_segments=50, figsize=(17,6))
res18A["pattern_1"]["spread"]


# In[118]:


df_pat = extract_patterns(seg_pat, line=80, dir_="A", min_share=0)
display(df_pat)  # Jupyter, or print(df_pat.to_string(index=False))


# In[149]:


# ==== 0) åªçœ‹ 2024/10 çš„è¦–åœ– ====
import numpy as np, pandas as pd, matplotlib.pyplot as plt
from collections import Counter

OCT_START = pd.Timestamp("2024-10-01")
NOV_START = pd.Timestamp("2024-10-31")

def make_october_views(seg, seg_src):
    # seg å·²æœ‰ link_start_timeï¼Œå¯ç›´æ¥åˆ‡
    S = seg[(seg["link_start_time"] >= OCT_START) & (seg["link_start_time"] < NOV_START)].copy()

    # seg_pat éœ€å¾ seg_src åšï¼ˆä¸çœ‹ anchorï¼‰ï¼Œæ™‚é–“ç”¨ã€Œèƒ½ä»£è¡¨å¯¦éš›æ—¥æœŸã€çš„æ¬„ä½ä¾†åˆ‡
    # ä¾å„ªå…ˆé †åºæŒ‘ä¸€å€‹å­˜åœ¨çš„æ™‚é–“æ¬„ä½
    for tc in ["DTEntreeFenetreArretReal","DTSortieFenetreArretReal",
               "DTEntreeArretAtp","DTSortieArretAtp","DTDepartTheo","DTArriveeTheo"]:
        if tc in seg_src.columns:
            base_time = seg_src[tc]
            break
    else:
        # æœ€å¾Œé€€ï¼šè‹¥åªæœ‰ DateCourseï¼ˆå­—ä¸²ï¼‰å°±è½‰æˆæ—¥æœŸ
        if "DateCourse" in seg_src.columns:
            base_time = pd.to_datetime(seg_src["DateCourse"], errors="coerce")
        else:
            raise ValueError("æ‰¾ä¸åˆ°èƒ½ç”¨ä¾†åˆ‡ 10 æœˆçš„æ™‚é–“æ¬„ä½")

    SS = seg_src[(base_time >= OCT_START) & (base_time < NOV_START)].copy()

    # å½¢æˆ seg_patï¼ˆä¿æŒä½ ä¹‹å‰çš„é‚è¼¯ï¼‰
    SS = SS.sort_values(["IdCourse","RangArretAsc"], kind="mergesort")
    SS["prev_CodeLong"] = SS.groupby("IdCourse")["CodeLong"].shift(1)
    seg_pat = SS.dropna(subset=["prev_CodeLong"]).copy()
    seg_pat["from_stop"] = seg_pat["prev_CodeLong"].astype(str).str.strip()
    seg_pat["to_stop"]   = seg_pat["CodeLong"].astype(str).str.strip()
    seg_pat["SegmentKey"] = seg_pat["from_stop"] + "â†’" + seg_pat["to_stop"]
    seg_pat["line"] = SS.get("C_Ligne").astype("Int64").astype(str)
    seg_pat["dir"]  = SS.get("C_SensAppl","NA").astype(str)

    return S, seg_pat

# ==== 1) ç”¨ 10 æœˆè³‡æ–™æ‰¾åˆ°æœ€å¸¸è¦‹å®Œæ•´è·¯å¾‘ï¼ˆå«èµ·é»ï¼‰ ====
def order_keys_by_top_pattern(seg_pat_oct, line, dir_=None, min_share=0.10):
    s = seg_pat_oct.copy()
    s = s[s["line"].astype(str) == str(line)]
    if dir_ is not None:
        s = s[s["dir"].astype(str) == str(dir_)]
    if s.empty:
        return []

    # ä»¥æ¯å€‹ç­æ¬¡çš„å®Œæ•´åºåˆ—ï¼ˆ[first_from] + list(to)ï¼‰ç•¶ pattern
    s = s.sort_values(["IdCourse","RangArretAsc"], kind="mergesort")
    def _trip_full_path(g):
        first_from = g["from_stop"].iloc[0]
        return tuple([first_from] + g["to_stop"].tolist())
    paths = s.groupby("IdCourse", sort=False).apply(_trip_full_path)

    cnt = Counter(paths)
    total = sum(cnt.values())
    if total == 0:
        return []

    pat, n = cnt.most_common(1)[0]
    share = n / total
    if share < min_share:
        # åˆ†äº«ç‡ä¸å¤ ä¹Ÿå…ˆå›å‚³ï¼ˆåªæ˜¯æé†’ï¼‰
        print(f"[warn] top pattern share only {share:.1%} (<{min_share:.0%})")

    ordered_keys = [f"{a}â†’{b}" for a,b in zip(pat[:-1], pat[1:])]
    return ordered_keys

# ==== 2A) å–®ä¸€ period168 çš„ boxplotï¼ˆçœ‹è©²æ™‚æ®µçš„é›¢æ•£ç¨‹åº¦ï¼‰ ====
def plot_box_by_period(S_oct, ordered_keys, period168, line, dir_=None,
                       min_n_per_seg=15, figsize=(17,6)):
    s = S_oct.copy()
    s = s[(s["line"].astype(str)==str(line)) & (s["period168"]==period168)]
    if dir_ is not None:
        s = s[s["dir"].astype(str)==str(dir_)]

    # åªç•™ >0 çš„ link_s
    s = s[pd.to_numeric(s["link_s"], errors="coerce") > 0]
    if s.empty:
        print("no data in this period"); return

    # ç¯©æ‰æ¨£æœ¬å¤ªå°‘çš„æ®µï¼ˆä½†ä¿ç•™é ­å…©æ®µï¼Œä»¥å…èµ·ç«™è¢«åƒæ‰ï¼‰
    vc = s["SegmentKey"].value_counts()
    head = set(ordered_keys[:2])
    keep = set(vc[vc>=min_n_per_seg].index) | (head & set(vc.index))
    keys = [k for k in ordered_keys if k in keep]
    if not keys:
        print("all segments < min_n in this period"); return

    data = [s.loc[s["SegmentKey"]==k, "link_s"].values for k in keys]

    # y è»¸ç”¨è©² period çš„ 1â€“99 ç™¾åˆ†ä½ï¼Œé¿å…é›¢ç¾¤å€¼æ‹‰æ‰¯
    y_lo = float(np.nanpercentile(s["link_s"], 1))
    y_hi = float(np.nanpercentile(s["link_s"], 99))

    plt.figure(figsize=figsize)
    bp = plt.boxplot(data, vert=True, showfliers=False, widths=0.6, labels=keys)
    plt.ylim(y_lo, y_hi)
    ttl = f"Line {line}"
    if dir_ is not None: ttl += f" | dir={dir_}"
    ttl += f" â€” period168={period168}"
    plt.title(ttl, fontsize=16)
    plt.ylabel("link_s (seconds)", fontsize=12)
    plt.grid(True, linestyle="--", alpha=0.3, axis="y")
    plt.xticks(rotation=70, ha="right")

    # åœ¨ x æ¨™ç±¤ä¸‹æ–¹æ¨™è¨»æ¯æ®µæ¨£æœ¬æ•¸
    for i, k in enumerate(keys, start=1):
        n_i = len(data[i-1])
        plt.text(i, y_lo + 0.02*(y_hi-y_lo), f"n={n_i}", ha="center", va="bottom", fontsize=9, rotation=90)

    plt.tight_layout(); plt.show()

# ==== 2B) 168Ã—segments ç†±åœ–ï¼šè®Šç•°åº¦ (p90 - p10) ====
def plot_heatmap_spread(S_oct, ordered_keys, line, dir_=None, min_n=15, vmax=None, figsize=(18,6)):
    s = S_oct.copy()
    s = s[s["line"].astype(str)==str(line)]
    if dir_ is not None:
        s = s[s["dir"].astype(str)==str(dir_)]
    s = s[pd.to_numeric(s["link_s"], errors="coerce") > 0]
    s = s[s["SegmentKey"].isin(set(ordered_keys))]

    g = s.groupby(["SegmentKey","period168"])["link_s"]
    agg = g.agg(n="size",
                p10=lambda x: np.nanpercentile(x,10),
                p90=lambda x: np.nanpercentile(x,90)).reset_index()
    agg.loc[agg["n"]<min_n, ["p10","p90"]] = np.nan
    agg["spread"] = agg["p90"] - agg["p10"]

    mat = (agg.pivot(index="period168", columns="SegmentKey", values="spread")
              .reindex(index=range(168), columns=ordered_keys))

    fig, ax = plt.subplots(figsize=figsize)
    im = ax.imshow(mat.to_numpy().T, aspect="auto", origin="upper",  # â† ä¸Šåœ¨ä¸Š
                   interpolation="nearest", vmin=0, vmax=vmax)
    fig.colorbar(im, ax=ax, label="spread (p90 - p10) [s]")
    ax.set_yticks(range(len(ordered_keys)))
    ax.set_yticklabels(ordered_keys, fontsize=9)
    ax.set_xticks(range(0,168,6)); ax.set_xticklabels(range(0,168,6))
    ttl = f"Line {line}" + (f" | dir={dir_}" if dir_ is not None else "") + " â€” variability heatmap (p90-p10)"
    ax.set_title(ttl, fontsize=16)
    ax.set_xlabel("period168 (Mon00=0 â€¦ Sun23=167)")
    ax.set_ylabel("segments (top â†’ bottom)")
    fig.tight_layout(); plt.show()

def plot_heatmap_spread_norm_per100m(S_oct, ordered_keys, line, dir_=None, min_n=15, vmax=None, figsize=(18,6)):
    s = S_oct.copy()
    s = s[(s["line"].astype(str)==str(line)) & (pd.to_numeric(s["link_s"], errors="coerce")>0)]
    if dir_ is not None:
        s = s[s["dir"].astype(str)==str(dir_)]
    s = s[s["SegmentKey"].isin(set(ordered_keys))]
    s = s[pd.to_numeric(s["distance_m"], errors="coerce")>0].copy()

    s["sec_per_100m"] = s["link_s"] / (s["distance_m"]/100.0)

    g = s.groupby(["SegmentKey","period168"])["sec_per_100m"]
    agg = g.agg(n="size",
                p10=lambda x: np.nanpercentile(x,10),
                p90=lambda x: np.nanpercentile(x,90)).reset_index()
    agg.loc[agg["n"]<min_n, ["p10","p90"]] = np.nan
    agg["spread"] = agg["p90"] - agg["p10"]   # å–®ä½ï¼šç§’/100m

    mat = (agg.pivot(index="period168", columns="SegmentKey", values="spread")
              .reindex(index=range(168), columns=ordered_keys))

    fig, ax = plt.subplots(figsize=figsize)
    im = ax.imshow(mat.to_numpy().T, aspect="auto", origin="upper",
                   interpolation="nearest", vmin=0, vmax=vmax)
    fig.colorbar(im, ax=ax, label="spread (p90 - p10) [sec/100m]")
    ax.set_yticks(range(len(ordered_keys))); ax.set_yticklabels(ordered_keys, fontsize=9)
    ax.set_xticks(range(0,168,6)); ax.set_xticklabels(range(0,168,6))
    ax.set_title(f"Line {line}" + (f" | dir={dir_}" if dir_ else "") + " â€” variability per distance", fontsize=16)
    ax.set_xlabel("period168"); ax.set_ylabel("segments (top â†’ bottom)")
    fig.tight_layout(); plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter
from matplotlib import colors

# ---- 1) æ‰¾ã€Œå®Œæ•´åœé åºåˆ—ã€çš„ top patternsï¼ˆåŒä½ ç¾åœ¨çš„ seg_pat æµç¨‹ï¼‰----
def _top_patterns(seg_line_dir, top_k=1, min_share=0.10):
    s = seg_line_dir.copy()
    if "RangArretAsc" in s.columns:
        s = s.sort_values(["IdCourse","RangArretAsc"], kind="mergesort")
    else:
        s = s.sort_values(["IdCourse","link_start_time"], kind="mergesort")
    if ("from_stop" not in s.columns) or ("to_stop" not in s.columns):
        a = s["SegmentKey"].str.split("â†’", expand=True)
        s["from_stop"] = a[0].astype(str).str.strip()
        s["to_stop"]   = a[1].astype(str).str.strip()
    else:
        s["from_stop"] = s["from_stop"].astype(str).str.strip()
        s["to_stop"]   = s["to_stop"].astype(str).str.strip()

    def _trip_full_path(g):
        first_from = g["from_stop"].iloc[0]
        to_seq     = g["to_stop"].tolist()
        return tuple([first_from] + to_seq)

    paths = s.groupby("IdCourse", sort=False).apply(_trip_full_path)
    cnt = Counter(paths); total = sum(cnt.values()) if cnt else 0
    ranked = [(p, n, n/total) for p, n in cnt.most_common() if total>0 and (n/total)>=min_share]
    return ranked[:top_k]
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter

# --- è·Ÿä½ ç¾æœ‰çš„ä¸€æ¨£ï¼šæ‰¾æœ€å¸¸è¦‹çš„å®Œæ•´åœé åºåˆ—ï¼ˆå«èµ·é»ï¼‰ ---
def _top_patterns(seg_line_dir, top_k=1, min_share=0.10):
    s = seg_line_dir.copy()
    if "RangArretAsc" in s.columns:
        s = s.sort_values(["IdCourse","RangArretAsc"], kind="mergesort")
    else:
        s = s.sort_values(["IdCourse","link_start_time"], kind="mergesort")
    if ("from_stop" not in s.columns) or ("to_stop" not in s.columns):
        a = s["SegmentKey"].str.split("â†’", expand=True)
        s["from_stop"] = a[0].astype(str).str.strip()
        s["to_stop"]   = a[1].astype(str).str.strip()
    else:
        s["from_stop"] = s["from_stop"].astype(str).str.strip()
        s["to_stop"]   = s["to_stop"].astype(str).str.strip()

    def _trip_full_path(g):
        first_from = g["from_stop"].iloc[0]
        to_seq     = g["to_stop"].tolist()
        return tuple([first_from] + to_seq)

    paths = s.groupby("IdCourse", sort=False).apply(_trip_full_path)
    cnt = Counter(paths); total = sum(cnt.values()) if cnt else 0
    ranked = [(p, n, n/total) for p, n in cnt.most_common() if total>0 and (n/total)>=min_share]
    return ranked[:top_k]

# --- p50 ç†±åœ–ï¼ˆå«è·é›¢æ ¡æ­£ç‰ˆæœ¬ï¼‰ ---
def plot_p50_heatmap_with_distance(
    seg, seg_pat, line, dir_="A",
    month_from="2024-10-01", month_to="2024-10-31",
    min_n=10, norm_unit_m=100,
    show_raw=True, show_norm=True,
    cmap_raw="magma", cmap_norm="cividis",
    figsize=(18,6)
):
    # 1) ç”¨ seg_pat æ±ºå®š row é †åºï¼ˆä¸æ¼æ‰èµ·ç«™ç¬¬ä¸€æ®µï¼‰
    s_pat = seg_pat[(seg_pat["line"].astype(str)==str(line)) & (seg_pat["dir"].astype(str)==str(dir_))].copy()
    pats = _top_patterns(s_pat, top_k=1, min_share=0.05)
    if not pats:
        print("No dominant pattern for this line/dir."); return
    pat, trips, share = pats[0]
    ordered_keys = [f"{a}â†’{b}" for a,b in zip(pat[:-1], pat[1:])]

    # 2) å–æŒ‡å®šæœˆä»½ + æœ‰æ•ˆ link_s çš„è³‡æ–™
    s = seg[(seg["line"].astype(str)==str(line)) & (seg["dir"].astype(str)==str(dir_))].copy()
    s = s[(s["link_start_time"]>=pd.to_datetime(month_from)) & (s["link_start_time"]<=pd.to_datetime(month_to))]
    s = s[pd.to_numeric(s["link_s"], errors="coerce") > 0]
    if s.empty:
        print("No positive link_s data in given month range."); return

    # 3) å„æ®µÃ—period168 çš„ p50 èˆ‡æ¨£æœ¬æ•¸
    g = (s.groupby(["SegmentKey","period168"])["link_s"]
           .agg(n="size", p50=lambda x: float(np.nanmedian(x)))).reset_index()
    g.loc[g["n"] < min_n, "p50"] = np.nan

    mat_raw = (g.pivot(index="SegmentKey", columns="period168", values="p50")
                 .reindex(index=ordered_keys))
    keep_rows = mat_raw.index[mat_raw.notna().any(axis=1)].tolist()
    mat_raw = mat_raw.loc[keep_rows]

    # 4) è·é›¢æ ¡æ­£ï¼šæ‹¿åŒä¸€æ®µåœ¨ seg ä¸­çš„è·é›¢ï¼ˆä¸­ä½æ•¸ï¼‰ï¼Œè¨ˆç®— ç§’/100m
    #    ç¼ºè·é›¢æˆ–<=0 çš„æ®µä¸€å¾‹ç•™ç™½
    dist_per_seg = (s.groupby("SegmentKey")["distance_m"]
                      .median().reindex(mat_raw.index))
    dist_vec = dist_per_seg.to_numpy()
    with np.errstate(divide='ignore', invalid='ignore'):
        mat_norm_vals = mat_raw.to_numpy() / (dist_vec[:, None] / float(norm_unit_m))
    mat_norm = pd.DataFrame(mat_norm_vals, index=mat_raw.index, columns=mat_raw.columns)
    mat_norm[(~np.isfinite(mat_norm)) | (dist_vec[:,None] <= 0)] = np.nan  # ç„¡è·é›¢â†’ç•™ç™½

    # 5) ç•«åœ–ï¼šæŒ‰éœ€æ±‚é¡¯ç¤º raw / norm
    n_panels = int(show_raw) + int(show_norm)
    fig, axs = plt.subplots(1, n_panels, figsize=figsize, squeeze=False)
    axs = axs[0]

    # x è»¸è¼”åŠ©
    def style_xaxis(ax):
        ax.set_xlim(-0.5, 167.5)
        for d in range(1,7):
            ax.axvline(d*24-0.5, color="k", lw=1, alpha=0.25)
        ax.set_xticks(np.arange(0, 168, 6))
        ax.set_xlabel("period168 (Mon00=0 â€¦ Sun23=167)")

    # y è»¸ï¼ˆä¸Šâ†’ä¸‹=è¡Œé§›æ–¹å‘ï¼‰
    def style_yaxis(ax, idx):
        ax.set_yticks(np.arange(len(idx)))
        ax.set_yticklabels(idx)
        ax.set_ylabel("segments (top â†’ bottom)")

    # è‰²éšç¯„åœï¼šç”¨ 1â€“99 åˆ†ä½ï¼Œé¿å…æ¥µç«¯å€¼
    vmin_raw = np.nanpercentile(mat_raw.values, 1) if show_raw else None
    vmax_raw = np.nanpercentile(mat_raw.values, 99) if show_raw else None
    vmin_norm = np.nanpercentile(mat_norm.values, 1) if show_norm else None
    vmax_norm = np.nanpercentile(mat_norm.values, 99) if show_norm else None

    pane = 0
    if show_raw:
        cmap = plt.get_cmap(cmap_raw).copy(); cmap.set_bad("white")
        im = axs[pane].imshow(mat_raw.values, aspect="auto", origin="upper",
                              interpolation="nearest", cmap=cmap,
                              vmin=vmin_raw, vmax=vmax_raw)
        style_xaxis(axs[pane]); style_yaxis(axs[pane], mat_raw.index)
        cbar = fig.colorbar(im, ax=axs[pane])
        cbar.set_label("median link_s (p50) [s]")
        axs[pane].set_title(f"Line {line} | dir={dir_} â€” p50 (seconds)  [{month_from[:7]}]")
        pane += 1

    if show_norm:
        cmap = plt.get_cmap(cmap_norm).copy(); cmap.set_bad("white")
        im = axs[pane].imshow(mat_norm.values, aspect="auto", origin="upper",
                              interpolation="nearest", cmap=cmap,
                              vmin=vmin_norm, vmax=vmax_norm)
        style_xaxis(axs[pane]); style_yaxis(axs[pane], mat_norm.index)
        cbar = fig.colorbar(im, ax=axs[pane])
        cbar.set_label(f"median link_s per {norm_unit_m} m (p50) [s/{norm_unit_m}m]")
        axs[pane].set_title(f"Line {line} | dir={dir_} â€” p50 (sec/{norm_unit_m}m)  [{month_from[:7]}]")

    plt.tight_layout()
    plt.show()

    return {
        "ordered_keys": ordered_keys,
        "pattern_share": share,
        "trips_in_pattern": trips,
        "mat_raw": mat_raw,
        "mat_norm": mat_norm,
        "dist_per_seg": dist_per_seg
    }


# In[152]:


# åªå– 2024/10 è¦–åœ–
S_oct, seg_pat_oct = make_october_views(seg, seg_src)

# å…ˆæ±ºå®š Line / Dir çš„é †åºï¼ˆç”¨ 10 æœˆ top patternï¼‰
ordered = order_keys_by_top_pattern(seg_pat_oct, line=18, dir_="A", min_share=0.10)
print(len(ordered), ordered[:5], "...")

# (A) æŒ‡å®šæŸå€‹ period168 çœ‹ã€Œè©²æ™‚æ®µã€çš„åˆ†å¸ƒ
plot_box_by_period(S_oct, ordered, period168= (2*24 + 8),  # ä¾‹ï¼šWed 08:00 â†’ 2*24+8=56
                   line=80, dir_="A", min_n_per_seg=10)

# (B) çœ‹æ•´å€‹ 168Ã—segments çš„è®Šç•°ç†±åœ–
plot_heatmap_spread(S_oct, ordered, line=18, dir_="A", min_n=10, vmax=120)


# In[153]:


plot_heatmap_spread_norm_per100m(S_oct, ordered, line=18, dir_="A", min_n=10, vmax=40.0)


# In[154]:


plot_p50_heatmap_with_distance(
        seg=seg, seg_pat=seg_pat, line=18, dir_="A",
        month_from="2024-10-01", month_to="2024-10-31",
        min_n=10, norm_unit_m=100,
        show_raw=True, show_norm=True,
        cmap_raw="magma", cmap_norm="cividis",
        figsize=(20,6)
)


# In[73]:


import numpy as np, pandas as pd
import matplotlib.pyplot as plt
from collections import Counter

def _top_patterns(seg_line_dir, top_k=1, min_share=0.10):
    s = seg_line_dir.copy()
    if "RangArretAsc" in s.columns:
        s = s.sort_values(["IdCourse","RangArretAsc"], kind="mergesort")
    else:
        s = s.sort_values(["IdCourse","link_start_time"], kind="mergesort")
    if "from_stop" not in s or "to_stop" not in s:
        a = s["SegmentKey"].str.split("â†’", expand=True)
        s["from_stop"] = a[0].astype(str).str.strip()
        s["to_stop"]   = a[1].astype(str).str.strip()

    def _trip_full_path(g):
        return tuple([g["from_stop"].iloc[0]] + g["to_stop"].tolist())

    paths = s.groupby("IdCourse", sort=False).apply(_trip_full_path)
    from collections import Counter
    cnt = Counter(paths); tot = sum(cnt.values())
    ranked = [(p,n,n/tot) for p,n in cnt.most_common() if tot>0 and n/tot>=min_share]
    return ranked[:top_k]

def plot_p50_heatmap_with_distance(seg, seg_pat, line, dir_="A",
                                   month_from="2024-10-01", month_to="2024-10-31",
                                   min_n=10, norm_unit_m=100,
                                   show_raw=True, show_norm=True,
                                   cmap_raw="magma", cmap_norm="cividis",
                                   figsize=(20,6)):
    # ---- ç¯©è³‡æ–™ï¼ˆåªçœ‹è©²æœˆï¼‰----
    s = seg[(seg["line"].astype(str)==str(line)) & (seg["dir"].astype(str)==str(dir_))].copy()
    s = s[(s["link_start_time"]>=pd.to_datetime(month_from)) &
          (s["link_start_time"]<=pd.to_datetime(month_to))].copy()

    # period168 ä¸€å®šè¦æ˜¯ intï¼Œä¸”è£œé½Š 0..167
    s["period168"] = (s["period168"].astype("Int64")).astype(int)

    # ä¾æœ€å¸¸è¦‹ pattern å–é †åºï¼ˆç”¨ seg_patï¼Œé¿å…èµ·ç«™æ‰ï¼‰
    sp0 = seg_pat[(seg_pat["line"].astype(str)==str(line)) & (seg_pat["dir"].astype(str)==str(dir_))].copy()
    pats = _top_patterns(sp0, top_k=1, min_share=0.05)
    if not pats:
        print("No dominant pattern."); return
    pat,_n,_share = pats[0][0], pats[0][1], pats[0][2]
    ordered_keys = [f"{a}â†’{b}" for a,b in zip(pat[:-1], pat[1:])]

    # åªä¿ç•™åœ¨ seg æœ‰æ¸¬å¾— link_s çš„æ®µï¼Œä¸”æ¨£æœ¬æ•¸ >= min_nï¼ˆä½†å‰å…©æ®µå¼·åˆ¶ä¿ç•™ï¼‰
    s = s[s["SegmentKey"].isin(set(ordered_keys))].copy()
    vc = s["SegmentKey"].value_counts()
    head_keep = set(ordered_keys[:2])
    keep = set(vc[vc>=min_n].index) | (head_keep & set(vc.index))
    ordered_keys = [k for k in ordered_keys if k in keep]
    s = s[s["SegmentKey"].isin(ordered_keys)].copy()

    # è·é›¢
    s["distance_m"] = pd.to_numeric(s["distance_m"], errors="coerce")

    # ---- èšåˆï¼šæ¯æ®µ Ã— period168 çš„ p50ï¼ˆç§’ï¼‰ï¼Œèˆ‡è·é›¢æ ¡æ­£ï¼ˆç§’/100mï¼‰----
    def _agg_one(df):
        out = {"p50": float(np.nanquantile(df["link_s"], 0.5))}
        # per 100mï¼šç”¨ã€Œç§’/100mã€çš„ä¸­ä½æ•¸ï¼ˆä¸­ä½æ•¸åœ¨æ­¤ä½œç”¨åƒ robust æ¯”ç‡ï¼‰
        m = (df["distance_m"]>0) & df["link_s"].notna()
        out["p50_per100m"] = float(np.nanquantile(df.loc[m,"link_s"] / (df.loc[m,"distance_m"]/norm_unit_m), 0.5)) if m.any() else np.nan
        return pd.Series(out)

    g = s.groupby(["SegmentKey","period168"]).apply(_agg_one).reset_index()

    # è½‰å¯¬ + è£œé½Š 0..167 æ¬„
    def _pivot(col):
        pv = g.pivot(index="SegmentKey", columns="period168", values=col)
        pv = pv.reindex(index=ordered_keys, columns=range(168))   # è£œé½Šæ¬„ã€ç¢ºä¿é †åº
        return pv

    H_raw  = _pivot("p50")
    H_norm = _pivot("p50_per100m")

    # ---- ç•«åœ–ï¼ˆç”¨ extent å°é½Š cellï¼Œvline åœ¨é‚Šç•Œ -0.5ï¼‰----
    nseg = len(ordered_keys)
    ncols = (show_raw and show_norm) + 1
    fig, axes = plt.subplots(1, ncols, figsize=figsize, constrained_layout=True)

    def _imshow(ax, M, ttl, cmap, cbar_label):
        im = ax.imshow(M.values, aspect="auto", origin="upper",
                       cmap=cmap, interpolation="nearest",
                       extent=[-0.5, 167.5, -0.5, nseg-0.5])
        # y tick
        ax.set_yticks(np.arange(nseg))
        ax.set_yticklabels(ordered_keys)
        ax.set_ylabel("segments (top â†’ bottom)")
        # x tickèˆ‡åˆ†éš”
        ax.set_xlabel("period168 (Mon00=0 ... Sun23=167)")
        for d in range(1,7):
            ax.axvline(d*24-0.5, color="white", lw=2, alpha=0.8)
        ax.set_title(ttl)
        cbar = fig.colorbar(im, ax=ax)
        cbar.set_label(cbar_label)
        return ax

    if show_raw and show_norm:
        _imshow(axes[0], H_raw,  f"Line {line} | dir={dir_} â€” p50 (seconds)   [{month_from[:7]}]",
                cmap_raw,  "median link_s (p50) [s]")
        _imshow(axes[1], H_norm, f"Line {line} | dir={dir_} â€” p50 (sec/{norm_unit_m}m)   [{month_from[:7]}]",
                cmap_norm, f"median link_s per {norm_unit_m} m (p50) [s/{norm_unit_m}m]")
    elif show_raw:
        _imshow(axes, H_raw,  f"Line {line} | dir={dir_} â€” p50 (seconds)   [{month_from[:7]}]",
                cmap_raw,  "median link_s (p50) [s]")
    else:
        _imshow(axes, H_norm, f"Line {line} | dir={dir_} â€” p50 (sec/{norm_unit_m}m)   [{month_from[:7]}]",
                cmap_norm, f"median link_s per {norm_unit_m} m (p50) [s/{norm_unit_m}m]")

    plt.show()


# In[74]:


plot_p50_heatmap_with_distance(
        seg=seg, seg_pat=seg_pat, line=80, dir_="A",
        month_from="2024-10-01", month_to="2024-10-31",
        min_n=10, norm_unit_m=100,
        show_raw=True, show_norm=True,
        cmap_raw="magma", cmap_norm="cividis",
        figsize=(20,6)
)


# In[75]:


import numpy as np, pandas as pd
from sklearn.metrics import mean_absolute_error

# ---- ä½ å·²ç¶“æœ‰çš„ï¼šLEVELSã€_fit_level_quantileã€fit_mapsã€apply_mapsã€make_empirical_baseline ----
# (ç•¥) ç›´æ¥æ²¿ç”¨ä½ ç¾æœ‰ç‰ˆæœ¬

def pinball_loss(y, qhat, alpha):
    m = np.isfinite(y) & np.isfinite(qhat)
    if not m.any(): return np.nan
    e = y[m] - qhat[m]
    return float(np.mean(np.maximum(alpha*e, (alpha-1)*e)))

def mae(y, yhat):
    m = np.isfinite(y) & np.isfinite(yhat)
    return float(np.mean(np.abs(y[m]-yhat[m]))) if m.any() else np.nan

def coverage(y, qhat):
    m = np.isfinite(y) & np.isfinite(qhat)
    return float(np.mean(y[m] <= qhat[m])) if m.any() else np.nan

# 1) æº–å‚™æ—¥æœŸåˆ‡é»ï¼ˆä»¥é€±ç‚ºä¾‹ï¼›è¦æ”¹æœˆä¹Ÿå¾ˆç°¡å–®ï¼‰
def weekly_cutpoints(s, min_weeks_train=4):
    dt = pd.to_datetime(s["link_start_time"])
    wk = (dt.dt.to_period("W").apply(lambda p: p.start_time)).rename("week")
    s = s.assign(_week=wk)
    weeks = sorted(s["_week"].unique())
    cuts = []
    for i in range(min_weeks_train, len(weeks)):
        train_to = weeks[i-1]            # åŒ…å«é€™ä¸€é€±
        test_week = weeks[i]             # é æ¸¬ä¸‹ä¸€é€±
        cuts.append((train_to, test_week))
    return s, cuts

# 2) å–®ä¸€å›åˆï¼šç”¢ç”Ÿ p50/p85 baseline ä¸¦è¨ˆç®—æŒ‡æ¨™ï¼ˆå«è·é›¢æ ¡æ­£ï¼‰
def eval_one_round(train_df, test_df, target="link_s", levels=None, q_main=0.50, q_hi=0.85):
    # fit maps
    b50, _, _, _ = make_empirical_baseline(train_df, test_df, target=target, q=q_main, levels=levels)
    b85, _, _, _ = make_empirical_baseline(train_df, test_df, target=target, q=q_hi,   levels=levels)

    y = test_df[target].to_numpy(dtype=float)
    d = test_df["distance_m"].to_numpy(dtype=float)

    out = {
        "MAE_p50": mae(y, b50),
        "MedAE_p50": float(np.nanmedian(np.abs(y - b50))),
        "Pinball@0.85": pinball_loss(y, b85, 0.85),
        "Coverage@0.85": coverage(y, b85),
    }
    # è·é›¢æ ¡æ­£
    m = np.isfinite(y) & np.isfinite(b50) & np.isfinite(d) & (d>0)
    out["MAE_p50_per100m"] = float(np.mean(np.abs(y[m]-b50[m]) / (d[m]/100))) if m.any() else np.nan
    out["MedAE_p50_per100m"] = float(np.nanmedian(np.abs(y[m]-b50[m]) / (d[m]/100))) if m.any() else np.nan
    return out, b50, b85

# 3) é€±åº¦æ»¾å‹•å›æ¸¬
def rolling_backtest(seg, target="link_s", levels=None, min_weeks_train=4):
    s, cuts = weekly_cutpoints(seg.dropna(subset=[target, "link_start_time"]), min_weeks_train)
    rows = []
    all_preds = []
    for train_to, test_week in cuts:
        tr = s[s["_week"] <= train_to].copy()
        te = s[s["_week"] == test_week].copy()
        if len(te)==0 or len(tr)==0: 
            continue
        met, p50, p85 = eval_one_round(tr, te, target=target, levels=levels)
        met["train_to"] = train_to; met["test_week"] = test_week; met["n_test"] = len(te)
        rows.append(met)
    res = pd.DataFrame(rows).sort_values("test_week")
    return res

# ---- è·‘èµ·ä¾†ï¼ˆlink_sï¼‰----
LEVELS = [
    ["SegmentKey","period168"],
    ["SegmentKey","is_weekend","hour"],
    ["SegmentKey"],
]
bt = rolling_backtest(seg, target="link_s", levels=LEVELS, min_weeks_train=4)

# æ•´é«”å›ç­”ï¼ˆå¹³å‡ & ä¸­ä½æ•¸ï¼‰
summary = bt.agg({
    "MAE_p50":["mean","median"],
    "MedAE_p50":["mean","median"],
    "Coverage@0.85":["mean","median"],
    "Pinball@0.85":["mean","median"],
    "MAE_p50_per100m":["mean","median"],
    "MedAE_p50_per100m":["mean","median"],
    "n_test":"sum",
})
print(summary)


# In[ ]:





# ### Dwell/Runtime segments

# In[119]:


import numpy as np
import pandas as pd
from collections import Counter

# ------------------------------
# Build window-based micro-segments (dwell + runtime)
# ------------------------------
def build_window_microsegments(seg_src, window_total_m=70):
    """
    Create a dataframe 'winseg' with dwell and runtime segments defined by window timestamps.
    Nodes are 'CodeLong|Entree' and 'CodeLong|Sortie'.
    Returns columns:
      - IdCourse, line, dir, trip_seq (ordering index)
      - from_node, to_node, type ('dwell'|'run'), duration_s, start_time, end_time
      - from_stop, to_stop, SegmentKey_win
      - distance_m (dwell=window_total_m; runtime=max(DistanceInterArret - window_total_m, 0) if available)
    """
    s = seg_src.copy()

    # ensure datetimes
    for c in ["DTEntreeFenetreArretReal","DTSortieFenetreArretReal","DTEntreeArretAtp","DTSortieArretAtp",
              "DTDepartTheo","DTArriveeTheo"]:
        if c in s.columns:
            s[c] = pd.to_datetime(s[c], errors="coerce")

    # basic ids
    if "RangArretAsc" in s.columns:
        s["RangArretAsc"] = pd.to_numeric(s["RangArretAsc"], errors="coerce")
    if "C_Ligne" in s.columns:
        s["C_Ligne"] = pd.to_numeric(s["C_Ligne"], errors="coerce")

    # sort trip order
    if "IdCourse" in s.columns:
        s = s.sort_values(["IdCourse","RangArretAsc"], kind="mergesort")
        g = s.groupby("IdCourse", sort=False)
    else:
        # fallback (line/date)
        s = s.sort_values(["C_Ligne","DateCourse","RangArretAsc"], kind="mergesort")
        g = s.groupby(["C_Ligne","DateCourse"], sort=False)

    # clean stop ids
    s["CodeLong"] = s.get("CodeLong","").astype(str).str.strip()
    s["line"] = s.get("C_Ligne").astype("Int64").astype(str) if "C_Ligne" in s.columns else "NA"
    s["dir"]  = s.get("C_SensAppl","NA").astype(str)

    # distances
    dist = pd.to_numeric(s.get("DistanceInterArret"), errors="coerce")

    # shift next stop's Entree for runtime construction
    s["next_Entree"]  = g["DTEntreeFenetreArretReal"].shift(-1)
    s["next_CodeLong"] = g["CodeLong"].shift(-1)

    # build dwell rows (where both window times exist)
    dwell_mask = s["DTEntreeFenetreArretReal"].notna() & s["DTSortieFenetreArretReal"].notna()
    dwell = s.loc[dwell_mask, [
        "IdCourse","line","dir","CodeLong","DTEntreeFenetreArretReal","DTSortieFenetreArretReal","RangArretAsc"
    ]].copy()
    dwell["from_node"] = dwell["CodeLong"] + "|Entree"
    dwell["to_node"]   = dwell["CodeLong"] + "|Sortie"
    dwell["type"] = "dwell"
    dwell["start_time"] = dwell["DTEntreeFenetreArretReal"]
    dwell["end_time"]   = dwell["DTSortieFenetreArretReal"]
    dwell["duration_s"] = (dwell["end_time"] - dwell["start_time"]).dt.total_seconds()
    dwell["from_stop"]  = dwell["CodeLong"]
    dwell["to_stop"]    = dwell["CodeLong"]
    dwell["SegmentKey_win"] = dwell["from_node"] + "â†’" + dwell["to_node"]
    dwell["distance_m"] = float(window_total_m)

    # QC dwell
    dwell.loc[(dwell["duration_s"]<=0) | (dwell["duration_s"]>900), "duration_s"] = np.nan

    # build runtime rows (need current Sortie and next Entree)
    run_mask = s["DTSortieFenetreArretReal"].notna() & s["next_Entree"].notna()
    run = s.loc[run_mask, [
        "IdCourse","line","dir","CodeLong","DTSortieFenetreArretReal","next_Entree","next_CodeLong","RangArretAsc"
    ]].copy()
    run["from_node"] = run["CodeLong"] + "|Sortie"
    run["to_node"]   = run["next_CodeLong"] + "|Entree"
    run["type"] = "run"
    run["start_time"] = run["DTSortieFenetreArretReal"]
    run["end_time"]   = run["next_Entree"]
    run["duration_s"] = (run["end_time"] - run["start_time"]).dt.total_seconds()
    run["from_stop"]  = run["CodeLong"]
    run["to_stop"]    = run["next_CodeLong"]
    run["SegmentKey_win"] = run["from_node"] + "â†’" + run["to_node"]

    # runtime distance: stop-to-stop minus window length (>=0). If DistanceInterArret missing, leave NaN.
    dist_runtime = pd.to_numeric(s.get("DistanceInterArret"), errors="coerce") - float(window_total_m)
    run["distance_m"] = dist_runtime.loc[run.index].clip(lower=0)

    # QC runtime
    run.loc[(run["duration_s"]<=0) | (run["duration_s"]>1800), "duration_s"] = np.nan

    # concatenate
    keep_cols = ["IdCourse","line","dir","RangArretAsc","from_node","to_node","type",
                 "start_time","end_time","duration_s","from_stop","to_stop","SegmentKey_win","distance_m"]
    winseg = pd.concat([dwell[keep_cols], run[keep_cols]], ignore_index=True)

    # an ordering index across micro-segments inside the trip
    # dwell at k gets seq=2*k, runtime k->k+1 gets seq=2*k+1 (preserves stop order)
    winseg["trip_seq"] = winseg.groupby("IdCourse")["start_time"].rank(method="first").astype(int)

    # convenience time bins
    winseg["period168"] = winseg["start_time"].dt.dayofweek * 24 + winseg["start_time"].dt.hour
    winseg["hour"] = winseg["start_time"].dt.hour
    winseg["dow"]  = winseg["start_time"].dt.dayofweek

    return winseg

# ------------------------------
# Pattern extraction on window-nodes
# ------------------------------
def extract_window_patterns(winseg, line, dir_=None, min_share=0.10):
    """
    Build full node sequence per trip: [A|Entree, A|Sortie, B|Entree, B|Sortie, ...]
    Count identical sequences; return dominant ones (>= min_share).
    """
    s = winseg[winseg["line"].astype(str)==str(line)].copy()
    if dir_ is not None:
        s = s[s["dir"].astype(str)==str(dir_)].copy()
    if s.empty:
        return pd.DataFrame(columns=["pattern_id","trips","share","n_nodes","start_node","end_node","pattern_str","preview"])

    s = s.sort_values(["IdCourse","trip_seq"], kind="mergesort")

    def _trip_node_seq(g):
        # reconstruct node sequence from from_node and to_node in order
        nodes = [g["from_node"].iloc[0]] + g["to_node"].tolist()
        return tuple(nodes)

    paths = s.groupby("IdCourse", sort=False).apply(_trip_node_seq)
    cnt = Counter(paths); total = sum(cnt.values())
    if total == 0:
        return pd.DataFrame(columns=["pattern_id","trips","share","n_nodes","start_node","end_node","pattern_str","preview"])

    rows = []
    ranked = [(p, n, n/total) for p, n in cnt.most_common() if (n/total) >= min_share]
    for i,(pat,n,share) in enumerate(ranked, start=1):
        pat_list = list(pat)
        preview = " â†’ ".join(pat_list[:5]) + (" â†’ â€¦ â†’ " + " â†’ ".join(pat_list[-4:]) if len(pat_list)>12 else "")
        rows.append({
            "pattern_id": i,
            "trips": n,
            "share": round(share, 4),
            "n_nodes": len(pat_list),
            "start_node": pat_list[0],
            "end_node": pat_list[-1],
            "pattern_str": " â†’ ".join(pat_list),
            "preview": preview
        })
    return pd.DataFrame(rows)

# ------------------------------
# Example usage
# ------------------------------
# winseg = build_window_microsegments(seg_src, window_total_m=70)
# df_winpat = extract_window_patterns(winseg, line=80, dir_="A", min_share=0.05)
# display(df_winpat.head(10))


# In[123]:


winseg = build_window_microsegments(seg_src, window_total_m=70)
df_winpat = extract_window_patterns(winseg, line=18, dir_="A", min_share=0.05)


# In[124]:


display(df_winpat.head(10))


# In[142]:


# ========= Imports =========
import numpy as np, pandas as pd, matplotlib.pyplot as plt
from collections import Counter

# ========= 0) Month slice for window segments =========
def month_slice_win(winseg, start="2024-10-01", end="2024-10-31"):
    S = winseg[(winseg["start_time"] >= pd.to_datetime(start)) &
               (winseg["start_time"] <  pd.to_datetime(end))].copy()
    return S

# ========= 1) Order keys by top *node* pattern, then convert to edge keys =========
def order_keys_by_top_window_pattern(
    S_month, line, dir_=None, min_share=0.05, type_filter="both"
):
    """
    returns ordered list of SegmentKey_win for the dominant node-sequence pattern.
    type_filter: 'both' | 'run' | 'dwell'
    """
    s = S_month[S_month["line"].astype(str)==str(line)].copy()
    if dir_ is not None: s = s[s["dir"].astype(str)==str(dir_)]
    if s.empty: return []

    # keep trip order
    s = s.sort_values(["IdCourse","trip_seq"], kind="mergesort")

    # full node sequence per trip (start node then successive to_nodes)
    def _trip_nodes(g):
        return tuple([g["from_node"].iloc[0]] + g["to_node"].tolist())
    paths = s.groupby("IdCourse", sort=False, group_keys=False).apply(_trip_nodes)

    cnt = Counter(paths); total = sum(cnt.values())
    if total == 0: return []

    # top pattern
    pat, n = cnt.most_common(1)[0]
    share = n/total
    if share < min_share:
        print(f"[warn] top pattern share only {share:.1%} (<{min_share:.0%})")

    # Convert the node pattern to *edge* keys and optionally filter by type
    # Recreate the trip rows that follow this exact node sequence
    # Build an edge-key list in order of appearance within that pattern
    nodes = list(pat)
    wanted_edges = set([f"{a}â†’{b}" for a, b in zip(nodes[:-1], nodes[1:])])

    sp = s[s["SegmentKey_win"].isin(wanted_edges)].copy()
    # enforce pattern order strictly
    sp["__ord"] = sp["from_node"].map({n:i for i,n in enumerate(nodes)})
    sp = sp.sort_values(["__ord"], kind="mergesort")

    if type_filter in ("run","dwell"):
        sp = sp[sp["type"] == type_filter]

    ordered_keys = sp["SegmentKey_win"].tolist()
    # remove consecutive duplicates (defensive)
    ordered_keys = [k for i,k in enumerate(ordered_keys) if i==0 or k != ordered_keys[i-1]]
    return ordered_keys

# ========= 2A) Boxplot for a single period168 =========
def plot_box_by_period_win(
    S_month, ordered_keys, period168, line, dir_=None, type_filter="run",
    min_n_per_seg=15, figsize=(17,6)
):
    s = S_month.copy()
    s = s[(s["line"].astype(str)==str(line)) & (s["period168"]==period168)]
    if dir_ is not None: s = s[s["dir"].astype(str)==str(dir_)]
    if type_filter in ("run","dwell"): s = s[s["type"]==type_filter]

    s = s[pd.to_numeric(s["duration_s"], errors="coerce") > 0]
    if s.empty: 
        print("no data in this period"); return

    vc = s["SegmentKey_win"].value_counts()
    head = set(ordered_keys[:2])
    keep = set(vc[vc>=min_n_per_seg].index) | (head & set(vc.index))
    keys = [k for k in ordered_keys if k in keep]
    if not keys:
        print("all segments < min_n in this period"); return

    data = [s.loc[s["SegmentKey_win"]==k, "duration_s"].values for k in keys]

    y_lo = float(np.nanpercentile(s["duration_s"], 1))
    y_hi = float(np.nanpercentile(s["duration_s"], 99))

    plt.figure(figsize=figsize)
    plt.boxplot(data, vert=True, showfliers=False, widths=0.6, labels=keys)
    plt.ylim(y_lo, y_hi)
    ttl = f"Line {line}" + (f" | dir={dir_}" if dir_ else "")
    ttl += f" â€” period168={period168} â€” {type_filter if type_filter!='both' else 'both types'}"
    plt.title(ttl, fontsize=16)
    plt.ylabel("duration_s (seconds)", fontsize=12)
    plt.grid(True, linestyle="--", alpha=0.3, axis="y")
    plt.xticks(rotation=70, ha="right")

    for i, k in enumerate(keys, start=1):
        n_i = len(data[i-1])
        plt.text(i, y_lo + 0.02*(y_hi-y_lo), f"n={n_i}", ha="center", va="bottom", fontsize=9, rotation=90)

    plt.tight_layout(); plt.show()

# ========= 2B) Heatmap of variability (p90 - p10) =========
def plot_heatmap_spread_win(
    S_month, ordered_keys, line, dir_=None, type_filter="run",
    min_n=15, vmax=None, figsize=(18,6)
):
    s = S_month.copy()
    s = s[s["line"].astype(str)==str(line)]
    if dir_ is not None: s = s[s["dir"].astype(str)==str(dir_)]
    if type_filter in ("run","dwell"): s = s[s["type"]==type_filter]
    s = s[pd.to_numeric(s["duration_s"], errors="coerce") > 0]
    s = s[s["SegmentKey_win"].isin(set(ordered_keys))]

    g = s.groupby(["SegmentKey_win","period168"])["duration_s"]
    agg = g.agg(n="size",
                p10=lambda x: np.nanpercentile(x,10),
                p90=lambda x: np.nanpercentile(x,90)).reset_index()
    agg.loc[agg["n"]<min_n, ["p10","p90"]] = np.nan
    agg["spread"] = agg["p90"] - agg["p10"]

    mat = (agg.pivot(index="period168", columns="SegmentKey_win", values="spread")
              .reindex(index=range(168), columns=ordered_keys))

    fig, ax = plt.subplots(figsize=figsize)
    im = ax.imshow(mat.to_numpy().T, aspect="auto", origin="upper",
                   interpolation="nearest", vmin=0, vmax=vmax)
    fig.colorbar(im, ax=ax, label="spread (p90 - p10) [s]")
    ax.set_yticks(range(len(ordered_keys))); ax.set_yticklabels(ordered_keys, fontsize=9)
    ax.set_xticks(range(0,168,6)); ax.set_xticklabels(range(0,168,6))
    ttl = f"Line {line}" + (f" | dir={dir_}" if dir_ else "") + f" â€” variability heatmap (p90-p10) â€” {type_filter}"
    ax.set_title(ttl, fontsize=16)
    ax.set_xlabel("period168 (Mon00=0 â€¦ Sun23=167)")
    ax.set_ylabel("segments (top â†’ bottom)")
    fig.tight_layout(); plt.show()

# ========= 2C) p50 heatmap (+ distance-normalized) =========
def plot_p50_heatmap_with_distance_win(
    S_month, ordered_keys, line, dir_=None, type_filter="run",
    min_n=10, norm_unit_m=100, show_raw=True, show_norm=True,
    cmap_raw="magma", cmap_norm="cividis", figsize=(20,6)
):
    s = S_month.copy()
    s = s[s["line"].astype(str)==str(line)]
    if dir_ is not None: s = s[s["dir"].astype(str)==str(dir_)]
    if type_filter in ("run","dwell"): s = s[s["type"]==type_filter]

    s = s[pd.to_numeric(s["duration_s"], errors="coerce") > 0]
    s = s[s["SegmentKey_win"].isin(set(ordered_keys))]
    if s.empty:
        print("No positive duration data in month range."); return

    g = (s.groupby(["SegmentKey_win","period168"])["duration_s"]
           .agg(n="size", p50=lambda x: float(np.nanmedian(x)))).reset_index()
    g.loc[g["n"]<min_n, "p50"] = np.nan

    mat_raw = (g.pivot(index="SegmentKey_win", columns="period168", values="p50")
                 .reindex(index=ordered_keys))
    keep_rows = mat_raw.index[mat_raw.notna().any(axis=1)].tolist()
    mat_raw = mat_raw.loc[keep_rows]

    # distance-normalized (sec per norm_unit_m)
    dist_per_seg = (s.groupby("SegmentKey_win")["distance_m"].median().reindex(mat_raw.index))
    dist_vec = dist_per_seg.to_numpy()
    with np.errstate(divide='ignore', invalid='ignore'):
        mat_norm_vals = mat_raw.to_numpy() / (dist_vec[:, None] / float(norm_unit_m))
    mat_norm = pd.DataFrame(mat_norm_vals, index=mat_raw.index, columns=mat_raw.columns)
    mat_norm[(~np.isfinite(mat_norm)) | (dist_vec[:,None] <= 0)] = np.nan

    n_panels = int(show_raw) + int(show_norm)
    fig, axs = plt.subplots(1, n_panels, figsize=figsize, squeeze=False); axs = axs[0]

    def style_x(ax):
        ax.set_xlim(-0.5, 167.5)
        for d in range(1,7): ax.axvline(d*24-0.5, color="k", lw=1, alpha=0.25)
        ax.set_xticks(np.arange(0,168,6)); ax.set_xlabel("period168 (Mon00 â€¦ Sun23)")

    def style_y(ax, idx):
        ax.set_yticks(np.arange(len(idx))); ax.set_yticklabels(idx); ax.set_ylabel("segments (top â†’ bottom)")

    pane = 0
    if show_raw:
        cmap = plt.get_cmap(cmap_raw).copy(); cmap.set_bad("white")
        vmin_raw = np.nanpercentile(mat_raw.values, 1); vmax_raw = np.nanpercentile(mat_raw.values, 95)
        im = axs[pane].imshow(mat_raw.values, aspect="auto", origin="upper",
                              interpolation="nearest", cmap=cmap, vmin=vmin_raw, vmax=vmax_raw)
        style_x(axs[pane]); style_y(axs[pane], mat_raw.index)
        fig.colorbar(im, ax=axs[pane]).set_label("median duration (p50) [s]")
        axs[pane].set_title(f"Line {line}" + (f" | dir={dir_}" if dir_ else "") + f" â€” p50 [s] â€” {type_filter}")
        pane += 1

    if show_norm:
        cmap = plt.get_cmap(cmap_norm).copy(); cmap.set_bad("white")
        vmin_norm = np.nanpercentile(mat_norm.values, 1); vmax_norm = np.nanpercentile(mat_norm.values, 95)
        im = axs[pane].imshow(mat_norm.values, aspect="auto", origin="upper",
                              interpolation="nearest", cmap=cmap, vmin=vmin_norm, vmax=vmax_norm)
        style_x(axs[pane]); style_y(axs[pane], mat_norm.index)
        fig.colorbar(im, ax=axs[pane]).set_label(f"median per {norm_unit_m} m (p50) [s/{norm_unit_m}m]")
        axs[pane].set_title(f"Line {line}" + (f" | dir={dir_}" if dir_ else "") + f" â€” p50 [s/{norm_unit_m}m] â€” {type_filter}")

    plt.tight_layout(); plt.show()


# In[131]:


# 1) month view (Oct 2024)
W_oct = month_slice_win(winseg, start="2024-10-01", end="2024-11-01")

# 2) decide row order by the dominant node pattern, then choose which micro-segments to display
ordered_all  = order_keys_by_top_window_pattern(W_oct, line=18, dir_="A", min_share=0.05, type_filter="both")
ordered_run  = order_keys_by_top_window_pattern(W_oct, line=18, dir_="A", min_share=0.05, type_filter="run")
ordered_dwel = order_keys_by_top_window_pattern(W_oct, line=18, dir_="A", min_share=0.05, type_filter="dwell")

print(len(ordered_run), ordered_run[:5], "...")


# In[138]:


# 3B) Variability heatmap (p90-p10)
plot_heatmap_spread_win(W_oct, ordered_run, line=18, dir_="A",
                        type_filter="run", min_n=10, vmax=100)


# In[144]:


plot_heatmap_spread_win(W_oct, ordered_dwel, line=18, dir_="A",
                        type_filter="dwell", min_n=10, vmax=100)


# In[155]:


# 3C) p50 heatmap (and per-distance)
plot_p50_heatmap_with_distance_win(W_oct, ordered_run, line=18, dir_="A",
                                   type_filter="run", min_n=10, norm_unit_m=100,
                                   show_raw=True, show_norm=True,
                                   cmap_raw="magma", cmap_norm="cividis", figsize=(20,6))

