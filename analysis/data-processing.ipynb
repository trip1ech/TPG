{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dcf8d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def detect_format(path: Path) -> str:\n",
    "    with path.open(encoding=\"utf-8-sig\") as f:\n",
    "        line1 = f.readline()\n",
    "        line2 = f.readline()\n",
    "    # Your fixed-width export has a second line made of dashes defining columns\n",
    "    if re.search(r\"-{3,}\", line2) and \",\" not in line2:\n",
    "        print('Detected fixed-width format based on second line dashes.')\n",
    "        return \"fwf\"\n",
    "\n",
    "    return \"csv\"\n",
    "\n",
    "def fwf_schema_from_header(path: Path):\n",
    "    with path.open(encoding=\"utf-8-sig\") as f:\n",
    "        header_line = f.readline().rstrip(\"\\n\")\n",
    "        dash_line = f.readline().rstrip(\"\\n\")\n",
    "\n",
    "    colspecs = [(m.start(), m.end()) for m in re.finditer(r\"-+\", dash_line)]\n",
    "    colspecs[-1] = (colspecs[-1][0], None)\n",
    "\n",
    "    raw_names = [header_line[s:] if e is None else header_line[s:e] for s, e in colspecs]\n",
    "\n",
    "    names, seen = [], {}\n",
    "    for nm in map(str.strip, raw_names):\n",
    "        seen[nm] = seen.get(nm, -1) + 1\n",
    "        names.append(nm if seen[nm] == 0 else f\"{nm}_{seen[nm]}\")\n",
    "    return colspecs, names\n",
    "\n",
    "\n",
    "def build_filtered_parquet(\n",
    "    raw_path: Path,\n",
    "    parquet_out: Path,\n",
    "    chunksize: int = 200_000,\n",
    "    filter_col: str = \"C_TypeAppl\",\n",
    "    filter_value=0,\n",
    "):\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "    fmt = detect_format(raw_path)\n",
    "    writer = None\n",
    "\n",
    "    if parquet_out.exists():\n",
    "        parquet_out.unlink()  # rebuild cleanly\n",
    "\n",
    "    if fmt == \"fwf\":\n",
    "        colspecs, names = fwf_schema_from_header(raw_path)\n",
    "        reader = pd.read_fwf(\n",
    "            raw_path,\n",
    "            colspecs=colspecs,\n",
    "            names=names,\n",
    "            skiprows=2,\n",
    "            na_values=[\"NULL\"],\n",
    "            encoding=\"utf-8-sig\",\n",
    "            chunksize=chunksize,\n",
    "        )\n",
    "    else:\n",
    "        SKIP_COLS = {\n",
    "            \"I_Distance\",\n",
    "            \"I_Duree\",\n",
    "            \"I_EstArretImprevu\",\n",
    "            \"I_HeureArret\",\n",
    "            \"I_HExploitArret\",\n",
    "            \"I_IdArretImprevu\",\n",
    "            \"I_Latitude\",\n",
    "            \"I_Longitude\",\n",
    "        }\n",
    "        \n",
    "        # If it's a real CSV, tune these for speed/memory:\n",
    "        reader = pd.read_csv(\n",
    "            raw_path,\n",
    "            low_memory=False,\n",
    "            chunksize=chunksize,\n",
    "            usecols=lambda c: c not in SKIP_COLS,\n",
    "            # usecols=[...],           # strongly recommended if you donâ€™t need all columns\n",
    "            # dtype={...},             # strongly recommended to avoid object dtype\n",
    "        )\n",
    "\n",
    "    total_in, total_out = 0, 0\n",
    "    for i, chunk in enumerate(reader, start=1):\n",
    "        total_in += len(chunk)\n",
    "\n",
    "        # Filter per chunk (avoid holding full df)\n",
    "        if filter_col in chunk.columns:\n",
    "            chunk = chunk[chunk[filter_col] == filter_value]\n",
    "        else:\n",
    "            raise KeyError(f\"Column {filter_col} not found in file columns: {list(chunk.columns)[:20]}...\")\n",
    "\n",
    "        total_out += len(chunk)\n",
    "\n",
    "        if len(chunk) == 0:\n",
    "            continue\n",
    "\n",
    "        table = pa.Table.from_pandas(chunk, preserve_index=False)\n",
    "\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(parquet_out, table.schema, compression=\"snappy\")\n",
    "        else:\n",
    "            table = table.cast(writer.schema, safe=False)\n",
    "            \n",
    "        writer.write_table(table)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"[chunk {i}] read={total_in:,} kept={total_out:,}\")\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "    print(f\"Done. Total read={total_in:,}, kept={total_out:,}. Parquet: {parquet_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "752fc83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[chunk 10] read=2,000,000 kept=1,995,215\n",
      "[chunk 20] read=4,000,000 kept=3,986,667\n",
      "[chunk 30] read=6,000,000 kept=5,981,477\n",
      "[chunk 40] read=8,000,000 kept=7,917,556\n",
      "[chunk 50] read=10,000,000 kept=9,838,948\n",
      "Done. Total read=11,704,166, kept=11,476,521. Parquet: /Users/ching-chichou/TPG/data/sae_arrets_full.parquet\n",
      "Loaded rebuilt parquet: (11476521, 100)\n",
      "   IdCourse  IdArret  RangArretAsc  RangArretDesc  DateCourse HDepartTheo  \\\n",
      "0  89604463        3            15             18  2025-11-01    00:19:09   \n",
      "1  89604463        3            15             18  2025-11-01    00:19:09   \n",
      "2  89604463        3            15             18  2025-11-01    00:19:09   \n",
      "\n",
      "  HArriveeTheo  DistanceTheo  TempsInterArretTheo  EcartDepart  ...  \\\n",
      "0     00:19:09          5502                   77           18  ...   \n",
      "1     00:19:09          5502                   77           18  ...   \n",
      "2     00:19:09          5502                   77           18  ...   \n",
      "\n",
      "   C_SensTheo  C_ServiceVoiture  C_TempsBattement  C_TempsDeviation  \\\n",
      "0           A              1-10                 0                 0   \n",
      "1           A              1-10                 0                 0   \n",
      "2           A              1-10                 0                 0   \n",
      "\n",
      "   C_TempsLigne  C_TheoriqueVsRealise  C_TypeAppl C_TypeTheo CodeLong  \\\n",
      "0          2385                     1           0          0   31DC00   \n",
      "1          2385                     1           0          0   31DC00   \n",
      "2          2385                     1           0          0   31DC00   \n",
      "\n",
      "         Arret  \n",
      "0  31 DECEMBRE  \n",
      "1  31 DECEMBRE  \n",
      "2  31 DECEMBRE  \n",
      "\n",
      "[3 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "HERE = Path.cwd().resolve()\n",
    "PROJECT_ROOT = HERE.parent\n",
    "RAW_BIG = PROJECT_ROOT / \"data\" / \"sae_arrets_full.csv\"   # change name\n",
    "PARQUET_CACHE = PROJECT_ROOT / \"data\" / \"sae_arrets_full.parquet\"\n",
    "\n",
    "FORCE_REBUILD_PARQUET = False\n",
    "\n",
    "if PARQUET_CACHE.exists() and not FORCE_REBUILD_PARQUET:\n",
    "    df = pd.read_parquet(PARQUET_CACHE)\n",
    "    print(\"Loaded cached parquet:\", df.shape)\n",
    "else:\n",
    "    build_filtered_parquet(RAW_BIG, PARQUET_CACHE, chunksize=200_000)\n",
    "    df = pd.read_parquet(PARQUET_CACHE)\n",
    "    print(\"Loaded rebuilt parquet:\", df.shape)\n",
    "\n",
    "print(df.head(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tpg-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
